import os
import streamlit as st
from dotenv import load_dotenv   
load_dotenv()                    

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# ================================
# è¨­å®šï¼ˆdotenv ã®å€¤ã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ï¼‰
# ================================
# .env ã‹ã‚‰ OPENAI_API_KEY ã‚’èª­ã¿è¾¼ã¿ã€ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆ
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Streamlit ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="LangChain Ã— Streamlit Demo",
    page_icon="ğŸ’¬",
    layout="centered"
)

# ================================
# å°‚é–€å®¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™é–¢æ•°
# ================================
def get_system_prompt(expert: str) -> str:
    if expert == "Aï¼šã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ":
        return (
            "ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã‚­ãƒ£ãƒªã‚¢å½¢æˆãƒ»åƒãæ–¹ãƒ»è»¢è·ãƒ»ã‚¹ã‚­ãƒ«æ§‹ç¯‰ã®è¦³ç‚¹ã‹ã‚‰å…·ä½“çš„ãªææ¡ˆã‚’ã—ã¦ãã ã•ã„ã€‚"
            "å°‚é–€å®¶ã¨ã—ã¦å„ªã—ãã€æ ¹æ‹ ã®ã‚ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )
    elif expert == "Bï¼šITã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ":
        return (
            "ã‚ãªãŸã¯çµŒé¨“è±Šå¯ŒãªITã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚"
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›¸è«‡ã«å¯¾ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã€ã‚¯ãƒ©ã‚¦ãƒ‰ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€æœ€é©ãªæŠ€è¡“é¸å®šã«ã¤ã„ã¦è«–ç†çš„ãªèª¬æ˜ã‚’ã—ã¦ãã ã•ã„ã€‚"
            "å°‚é–€çš„ã ãŒåˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã‚’ç”¨ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        )
    else:
        return "ã‚ãªãŸã¯ä¸å¯§ã«å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"


# ================================
# LLM å®Ÿè¡Œé–¢æ•°ï¼ˆè¦ä»¶ã®é–¢æ•°ï¼‰
# ================================
def run_llm(user_text: str, expert: str) -> str:
    """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶ã®ç¨®é¡ã‚’åŸºã« LLM ã®å›ç­”ã‚’è¿”ã™é–¢æ•°"""

    system_prompt = get_system_prompt(expert)

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input_text}")
        ]
    )

    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.5
    )

    chain = prompt | llm

    response = chain.invoke({"input_text": user_text})
    return response.content

# ================================
# UI è¡¨ç¤º
# ================================
st.title("ğŸ’¬ LangChain Ã— Streamlit Demo App")

st.markdown("""
### ğŸ” ã‚¢ãƒ—ãƒªã®æ¦‚è¦
ã“ã®ã‚¢ãƒ—ãƒªã¯ LangChain ã‚’åˆ©ç”¨ã—ã¦ LLM ã«è³ªå•ã‚’æŠ•ã’ã‹ã‘ã‚‹ãƒ‡ãƒ¢ã§ã™ã€‚

- ä»»æ„ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›  
- å°‚é–€å®¶ã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ  
- é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã§å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™  

LangChain ã® **ChatPromptTemplate + LLMChain** ã‚’ä½¿ã£ã¦ LLM ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚

### ğŸ§ª ä½¿ã„æ–¹
1. å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã¶  
2. è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›  
3. ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™  
4. ç”»é¢ä¸‹ã«å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™  
""")

# ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ï¼ˆå°‚é–€å®¶é¸æŠï¼‰
expert_type = st.radio(
    "å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š",
    ["Aï¼šã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ", "Bï¼šITã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ"],
)

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
input_text = st.text_area("è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", height=150)

# ãƒœã‚¿ãƒ³
if st.button("é€ä¿¡"):
    if input_text.strip():
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
            answer = run_llm(input_text, expert_type)
        st.subheader("ğŸ§  å›ç­”çµæœ")
        st.write(answer)
    else:
        st.warning("å…¥åŠ›å†…å®¹ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚")
